{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "****91. Boltzmann Machine****"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Boltzmann Machine is a neural network that's not feed forward\n",
    "like other neural networks for example ANN, CNN, RNN...\n",
    "\n",
    "Boltzmann machine doesn't have output layer.\n",
    "\n",
    "The Boltzmann machine network is a full graph that is each node is connected to every other node\n",
    "in a bidirectional connection.\n",
    "part of the nodes there's visible nodes and part is hidden nodes.\n",
    "\n",
    "That's because a boltzmann machine simulates a whole system\n",
    "and each node is a parameter in the system,\n",
    "it generates data and states that the system could be in.\n",
    "\n",
    "The example that was given in the lectures is a nuclear power plant,\n",
    "and there's a node that represents the turbine RPMs,\n",
    "other node represents the temperature of the water that's being pumped in.\n",
    "\n",
    "And if we want to simulate a meltdown there isn't enough data of what a meltdown would look like,\n",
    "so the dataset of the power plant states would be imbalanced\n",
    "and there's a lot of states that the power plant is in good state and very few of a meltdown,\n",
    "so we can't throw a ML model on it to learn what a meltdown is.\n",
    "\n",
    "A boltzmann machine is continuously generates states.\n",
    "    state 1 -> state 2 -> ... -> state n\n",
    "\n",
    "according to the data we feed it to the Boltzmann Machine it learns what's the relation\n",
    "between the nodes and the constraints between them.\n",
    "\n",
    "after the training phase, the machine would become a model that represents\n",
    "the system that we have the data on, and since we trained the model on a good behaviour of the system,\n",
    "then we know what's normal behaviour of the system, and can detect an abnormal behaviour of the system\n",
    "\n",
    "in situations where getting certain type of data is extremely expensive in resources,\n",
    "unsupervised learning such as boltzmann machine is preferred over supervised learning\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "****92. Energy-Based Models (EBM)****"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start with the equation Boltzmann distribution (Softmax function)\n",
    "\n",
    "    $$ \\large p_i = \\frac{e^{-{\\epsilon_i}/kT}}{\\sum\\limits_{j=1}^{M} e^{-{\\epsilon_j}/kT}}$$\n",
    "\n",
    "I've seen this equation in different places and contexts\n",
    "1. in ML2, this is the softmax function\n",
    "applied on the vector epsilon / kT where epsilon is a vector\n",
    "\n",
    "2. in AI, in RL when we talked about Boltzmann policy (GLIE) where instead of -epsilon/k there's\n",
    "the Q function\n",
    "\n",
    "Example to explain the distribution, imagine there's a room full of gas,\n",
    "why the gas is distributed across the room? why not all the gas be in the same corner?\n",
    "statistically there's a non-zero probability for that to happen, and according\n",
    "to the boltzmann distribution, since the gas would have a lot of energy in that state\n",
    "the probability would be extremely low.\n",
    "the lowest energy state will have the highest probability.\n",
    "\n",
    "energy defined in boltzmann machine through the weights, and once the system is trained\n",
    "the system, that's based on the weights, will always try to find the lowest energy state\n",
    "\n",
    "example of an equation of restricted boltzmann machine\n",
    "\n",
    "    $$ \\large E(v, h) = - \\sum_{i} a_{i}v_{i} - \\sum_{j} b_{j}h_{j} - \\sum_{i}\\sum_{j} v_{i}w_{i, j}h_{j}$$\n",
    "\n",
    "$a_i, b_i$ are biases of the system\n",
    "$v_i$ is the i-th visible node\n",
    "$h_i$ is the i-th hidden node\n",
    "$w_{i,j}$ is the connection between the i-th visible node and the j-th hidden node\n",
    "\n",
    "    $$ \\large P(v, h) = \\frac{e^{-E(v, h)}}{Z}$$\n",
    "\n",
    "$Z$ is the sum of all the exponents across all the possible states\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "****94. Restricted Boltzmann Machine (RBM)****"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In theory the standard boltzmann machine is a great model but in practice\n",
    "it's hard to implement the boltzmann machine since the connections grow exponentially\n",
    "so a different type of architecture was proposed and that is **Restricted Boltzmann Machine**, and the restriction is that hidden nodes can't connect to each other and the visible node can't connect to each other.\n",
    "\n",
    "A RBM could work as a recommendation system as in the example that was\n",
    "given in the lecture\n",
    "\n",
    "in the example there's two layers\n",
    "hidden Nodes: 5 nodes connected to all the visible nodes\n",
    "Visible nodes: 6 nodes connected to all the hidden nodes\n",
    "\n",
    "this model keeps on generating data, it's trained through a process **contrastive divergence**, through that process the RBM become more and more like the recommendation system that it's trained on it's data.\n",
    "\n",
    "\n",
    "Suppose the Boltzmann machine is already trained up\n",
    "how does it work?\n",
    "\n",
    "well in the example that's been given, for our understanding, we suppose that the\n",
    "hidden nodes picked up on some features that's\n",
    "\n",
    "<pre style=\"font-size:12px\">\n",
    "                Drama    Action    Dicaprio    Oscar    Tarantino\n",
    "hidden Nodes      @        @          @          @          @\n",
    "                  |\\      /|\\        /|\\        /|\\        /|\\\n",
    "                                     ...\n",
    "                  |/      \\|/        \\|/        \\|/        \\|/     \\|\n",
    "Visible Nodes     @        @          @          @          @       @\n",
    "\n",
    "new data         The      Fight    Forrest      Pulp      Titanic   The\n",
    "                matrix    club      Gump        Fiction             Departed\n",
    "                  0        _         1            0          1         _\n",
    "</pre>\n",
    "\n",
    "that is they've seen the matrix and didn't like it, seen Titanic and liked it...\n",
    "and didn't see Fight Club and The Departed\n",
    "we want to know if they'd like Fight Club or The Departed\n",
    "\n",
    "the drama node would light up green since Forrest Gump and Titanic are both drama\n",
    "the action node would light up red since the matrix and pulp fiction was given\n",
    "the rating 0\n",
    "the dicaprio and the Oscar nodes would also light up green\n",
    "and the Tarantino node would light up red since pulp fiction was given 0 rating\n",
    "\n",
    "after this a backward pass will be activated and the boltzmann machine would try and reconstruct the data point, and that process also happens in the training,\n",
    "the machine tries and reconstruct the data, similar to how VAE is trained to capture the hidden space of the data (was taught in ML2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "****95. Contrastive Divergence****"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we don't have a directed network like we saw in the previous parts of the course (ANNs, CNNs, RNNs...), but instead the boltzmann machine in an undirected network\n",
    "We can't adjust the weights through backpropagation and gradient descent\n",
    "\n",
    "And so the Contrastive Divergence solves this problem\n",
    "\n",
    "So first the weights are generated randomly, and then we'll calculate the hidden nodes, then the hidden nodes will try and reconstruct the input using the same weights.\n",
    "key concept is that the reconstructed visible node will not be equal to the initial input\n",
    "and that's because the reconstruction process involves all the hidden nodes. and each of the hidden nodes was constructed using all the visible nodes.\n",
    "and so the reconstruction a visible node will be generated from other visible nodes even though it's not connected to other visible nodes directly.\n",
    "\n",
    "let's take this example:\n",
    "<pre style=\"font-size:12px\">\n",
    "                        h1      h2      h3\n",
    "Hidden nodes:           @       @       @\n",
    "                        |\\     /|\\     /|\n",
    "                               ...\n",
    "                            |/     \\|\n",
    "visible nodes:              @       @\n",
    "                            v1      v2\n",
    "\n",
    "phase i:\n",
    "    h1 will be constructed from a combination of v1, v2\n",
    "    h2 will also be constructed from a combination of v1, v2\n",
    "    and so is h3\n",
    "\n",
    "    then\n",
    "    v1, v2 will be reconstructed using h1, h2, h3\n",
    "\n",
    "that back and forth process of construction and reconstruct will converge at some point to the weights that regenrates the input\n",
    "\n",
    "</pre>\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial log p(V^0)}{\\partial w_{i,j}} = <v_i^0h_i^0>-<v_i^\\infty h_i^\\infty>$$\n",
    "\n",
    "the derivative of the log probability of certain state $V^0$ w.r.t the weight $w_{i,j}$\n",
    "\n",
    "\n",
    "Papers to read:\n",
    "- A fast learning algorithm for deep belied nets By Geoffrey Hinton\n",
    "- Notes on Contrastive Divergence By Oliver Woodford\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "****96. Deep Belief Networks (DBNs)****"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A deep belief network is a stacked Boltzmann machines on top of each other.\n",
    "\n",
    "<pre style=\"font-size:12px\">\n",
    "\n",
    "                                     ...\n",
    "hidden Nodes      @        @          @          @          @\n",
    "                  |\\      /|\\        /|\\        /|\\        /|\\\n",
    "                                     ...\n",
    "                  |/      \\|/        \\|/        \\|/        \\|/\n",
    "hidden Nodes      @        @          @          @          @\n",
    "                  |\\      /|\\        /|\\        /|\\        /|\\\n",
    "                                     ...\n",
    "                  |/      \\|/        \\|/        \\|/        \\|/     \\|\n",
    "Visible Nodes     @        @          @          @          @       @\n",
    "</pre>\n",
    "\n",
    "***Important thing about DBNs***\n",
    "the connection between all the layers except the last two are directed downward.\n",
    "that is the connections between the last two layers are undirected and all the other connections are directed downward\n",
    "(see more <a href=\"https://en.wikipedia.org/wiki/Deep_belief_network\">here</a>)\n",
    "\n",
    "There's two methods for training DBNs\n",
    "1. Greedy layer-wise training: trains the model layer by layer, whereas each layer is treated as an RBM\n",
    "2. Wake-Sleep algorithm: train all the way up then train all the way down.\n",
    "\n",
    "\n",
    "Papers on DBNs\n",
    "- Greedy Layer-Wise Training of Deep Networks by Yoshua Bengio et al. (2006)\n",
    "- The wake-sleep algorithm for unsupervised neural networks By Geoffrey Hinton et al. (1995)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "****97. Deep Boltzmann Machines (DBMs)****"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "first things first: DBN != DBM\n",
    "\n",
    "The main difference between DBNs and DBMs is that in DBNs the connections are directed downward except the connections between the last two layers are undirected,\n",
    "and in DBMs all the connections are undirected.\n",
    "\n",
    "**Paper to read**\n",
    "Deep Boltzmann Machines By Ruslan Salakhutdinov et al. (2009)\n",
    "\n",
    "it's said that DBMs can extract features more complex that the DBNs can extract\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
